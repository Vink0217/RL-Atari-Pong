# --- General Settings ---
# Space Invaders configuration modeled on pacman.yaml
env_id: "ALE/SpaceInvaders-v5"
base_log_dir: "./runs"
total_timesteps: 10000000  # You can override this with --total-timesteps
n_envs: 64  # Optimized for high-throughput training
frame_stack: 4
seed: 0
device: "cuda"        # Default to GPU training
use_subproc: true    # Use parallel env processing

# --- Algorithm Selection ---
# This is the key read by runner.py.
algorithm: "PPO"

# --- Algorithm-Specific Hyperparameters ---
# runner.py will read the section that matches the 'algorithm' key
algo_params:
  PPO:
    # Hyperparameters specific to PPO (optimized for throughput)
    policy: "CnnPolicy"
    learning_rate: 2.5e-4
    n_steps: 256      # Larger steps for better trajectory collection
    batch_size: 2048  # Large batch for maximum GPU utilization
    n_epochs: 4
    gamma: 0.99
    ent_coef: 0.01
    clip_range: 0.1
    gae_lambda: 0.95
    normalize_advantage: true

  DQN:
    # Hyperparameters specific to DQN
    policy: "CnnPolicy"
    learning_rate: 0.0001
    buffer_size: 100000
    learning_starts: 10000
    batch_size: 32
    tau: 1.0
    gamma: 0.99
    train_freq: 4
    gradient_steps: 1
    target_update_interval: 1000
    exploration_fraction: 0.1
    exploration_final_eps: 0.01

  A2C:
    # Hyperparameters specific to A2C
    policy: "CnnPolicy"
    learning_rate: 0.0007
    n_steps: 5
    gamma: 0.99
    ent_coef: 0.01
    vf_coef: 0.5
    
# --- Callback Settings ---
# These are general, so they stay at the top level
checkpoint_freq: 100000
eval_freq: 200000
n_eval_episodes: 10

early_stop:
  enabled: false
  patience: 5
  min_delta: 1.0
